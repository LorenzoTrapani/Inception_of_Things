# Design Document - P1: K3s and Vagrant Multi-Node Cluster

## Overview

Il progetto P1 implementa un cluster Kubernetes multi-node usando K3s su due VM Vagrant. Questo è un setup classico per apprendere Kubernetes: un nodo controller che gestisce il control plane e un nodo worker che esegue i workload.

**Architettura generale:**
- 2 VM su rete privata (192.168.56.0/24)
- Controller node (lotrapanS) con K3s server
- Worker node (lotrapanSW) con K3s agent
- Token sharing tramite Vagrant shared folder
- Flannel CNI per networking tra pod
- kubectl configurato su entrambi i nodi

**Obiettivi didattici:**
- Capire l'architettura multi-node di Kubernetes
- Imparare come i nodi joinano un cluster
- Configurare networking e CNI
- Gestire autenticazione tra nodi

## Architecture

### Infrastructure Layer

```
Host Machine (Linux)
    │
    └── VirtualBox
            │
            ├── VM 1: lotrapanS (Controller)
            │       ├── IP: 192.168.56.110
            │       ├── OS: Ubuntu 22.04
            │       ├── RAM: 1024 MB
            │       ├── CPU: 1
            │       └── Role: K3s Server (Control Plane)
            │
            └── VM 2: lotrapanSW (Worker)
                    ├── IP: 192.168.56.111
                    ├── OS: Ubuntu 22.04
                    ├── RAM: 1024 MB
                    ├── CPU: 1
                    └── Role: K3s Agent (Worker)
```

### Network Layer

```
Private Network: 192.168.56.0/24
    │
    ├── 192.168.56.110 (lotrapanS)
    │       ├── K3s API Server: port 6443
    │       ├── Flannel VXLAN: port 8472
    │       └── Kubelet: port 10250
    │
    └── 192.168.56.111 (lotrapanSW)
            ├── Kubelet: port 10250
            └── Flannel VXLAN: port 8472

Vagrant Shared Folder: /vagrant
    ├── token (generated by controller)
    └── kubeconfig (generated by controller)
```

### Kubernetes Layer

```
K3s Cluster
    │
    ├── Control Plane (lotrapanS)
    │       ├── kube-apiserver
    │       ├── kube-controller-manager
    │       ├── kube-scheduler
    │       ├── etcd (embedded)
    │       └── kubelet
    │
    └── Worker Node (lotrapanSW)
            ├── kubelet
            └── kube-proxy

CNI: Flannel (VXLAN mode)
    ├── Pod CIDR: 10.42.0.0/16
    └── Service CIDR: 10.43.0.0/16
```

### Provisioning Flow

```
1. Vagrant Up
    │
    ├──> Create lotrapanS VM
    │       │
    │       ├──> Run inline provisioner (update, install netcat, setup /etc/hosts)
    │       │
    │       └──> Run k3s_master.sh
    │               ├──> Detect network interface
    │               ├──> Install K3s server
    │               ├──> Setup kubectl for vagrant user
    │               ├──> Generate and save token to /vagrant/token
    │               └──> Save kubeconfig to /vagrant/kubeconfig
    │
    └──> Create lotrapanSW VM
            │
            ├──> Run inline provisioner (update, install netcat, setup /etc/hosts)
            │
            └──> Run k3s_agent.sh
                    ├──> Wait for /vagrant/token (max 300s)
                    ├──> Read token
                    ├──> Detect network interface
                    ├──> Install K3s agent with token and server URL
                    └──> Copy kubeconfig from /vagrant/kubeconfig

2. Cluster Ready
    │
    └──> Both nodes in Ready state
            ├──> lotrapanS: control-plane,master
            └──> lotrapanSW: <none>
```

## Components and Interfaces

### 1. Vagrantfile

**Responsabilità:**
- Definire la configurazione delle due VM
- Configurare la rete privata con IP statici
- Orchestrare il provisioning in ordine corretto
- Gestire il ciclo di vita delle VM

**Configurazione:**
```ruby
# Global provisioner (runs on both VMs)
- Update apt packages
- Install netcat-openbsd (for network testing)
- Setup /etc/hosts with both hostnames

# Controller Node (lotrapanS)
- Box: bento/ubuntu-22.04
- Hostname: lotrapanS
- IP: 192.168.56.110
- RAM: 1024 MB
- CPU: 1
- Provisioner: scripts/k3s_master.sh

# Worker Node (lotrapanSW)
- Box: bento/ubuntu-22.04
- Hostname: lotrapanSW
- IP: 192.168.56.111
- RAM: 1024 MB
- CPU: 1
- Provisioner: scripts/k3s_agent.sh (with arg: 192.168.56.110)
```

**Design decisions:**
- Controller viene creato per primo (Vagrant ordine alfabetico: lotrapanS < lotrapanSW)
- Worker riceve l'IP del master come argomento
- Shared folder /vagrant è automatico in Vagrant
- Inline provisioner per setup comune evita duplicazione

### 2. K3s Master Installation Script (k3s_master.sh)

**Responsabilità:**
- Rilevare l'interfaccia di rete corretta
- Installare K3s in modalità server
- Configurare kubectl per l'utente vagrant
- Generare e salvare il token per il worker
- Salvare kubeconfig nella shared folder

**Implementazione dettagliata:**

```bash
#!/bin/bash

# Step 1: Detect network interface
# Cerca l'interfaccia che ha l'IP 192.168.56.110
IFACE=$(ip -4 addr show | grep "192.168.56.110" | awk '{print $NF}')

# Step 2: Install K3s with appropriate flags
if [ -z "$IFACE" ]; then
  # No interface detected, use default
  echo "Interface with IP 192.168.56.110 not found, using default"
  curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --node-ip=192.168.56.110 --write-kubeconfig-mode=644" sh -
else
  # Interface detected, configure Flannel explicitly
  echo "Using interface: $IFACE"
  curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --node-ip=192.168.56.110 --flannel-iface=$IFACE --write-kubeconfig-mode=644" sh -
fi

# Step 3: Setup kubectl for vagrant user
sudo mkdir -p /home/vagrant/.kube
sudo cp /etc/rancher/k3s/k3s.yaml /home/vagrant/.kube/config
sudo sed -i 's/127.0.0.1/192.168.56.110/g' /home/vagrant/.kube/config
sudo chown -R vagrant:vagrant /home/vagrant/.kube

# Step 4: Extract and save token
K3S_TOKEN=$(sudo cat /var/lib/rancher/k3s/server/node-token)
echo $K3S_TOKEN > /vagrant/token

# Step 5: Save kubeconfig to shared folder
sudo cp /home/vagrant/.kube/config /vagrant/kubeconfig
sudo chmod 644 /vagrant/kubeconfig

echo "K3s server ready, token and kubeconfig saved"
```

**Design decisions:**
- `--node-ip`: Specifica l'IP da usare per il nodo (importante per multi-homed hosts)
- `--flannel-iface`: Dice a Flannel quale interfaccia usare per VXLAN
- `--write-kubeconfig-mode=644`: Permette lettura senza sudo
- `sed` per sostituire 127.0.0.1 con IP reale: permette accesso remoto
- Token salvato in /vagrant: accessibile dal worker tramite shared folder

**Perché rilevare l'interfaccia:**
VirtualBox crea multiple interfacce di rete:
- eth0: NAT (per accesso internet)
- eth1: Private network (192.168.56.x)

Flannel deve usare eth1 per comunicazione tra nodi, altrimenti usa eth0 e i pod non comunicano.

### 3. K3s Agent Installation Script (k3s_agent.sh)

**Responsabilità:**
- Attendere la disponibilità del token
- Rilevare l'interfaccia di rete corretta
- Installare K3s in modalità agent
- Configurare kubectl per l'utente vagrant

**Implementazione dettagliata:**

```bash
#!/bin/bash
MASTER_IP=$1

# Step 1: Wait for token with timeout
TIMEOUT=300
ELAPSED=0
while [ ! -f /vagrant/token ] && [ $ELAPSED -lt $TIMEOUT ]; do
  echo "Waiting for master token... ($ELAPSED/$TIMEOUT seconds)"
  sleep 5
  ELAPSED=$((ELAPSED + 5))
done

# Step 2: Check if token was found
if [ ! -f /vagrant/token ]; then
  echo "Timeout waiting for master token"
  exit 1
fi

# Step 3: Read token
K3S_TOKEN=$(cat /vagrant/token)
echo "Token found, connecting to master at $MASTER_IP"

# Step 4: Detect network interface
IFACE=$(ip -4 addr show | grep "192.168.56.111" | awk '{print $NF}')

# Step 5: Install K3s agent with appropriate flags
if [ -z "$IFACE" ]; then
  echo "Interface with IP 192.168.56.111 not found, using default"
  curl -sfL https://get.k3s.io | K3S_URL=https://$MASTER_IP:6443 K3S_TOKEN=$K3S_TOKEN INSTALL_K3S_EXEC="agent --node-ip=192.168.56.111" sh -
else
  echo "Using interface: $IFACE"
  curl -sfL https://get.k3s.io | K3S_URL=https://$MASTER_IP:6443 K3S_TOKEN=$K3S_TOKEN INSTALL_K3S_EXEC="agent --node-ip=192.168.56.111 --flannel-iface=$IFACE" sh -
fi

# Step 6: Setup kubectl for vagrant user
sudo mkdir -p /home/vagrant/.kube
sudo cp /vagrant/kubeconfig /home/vagrant/.kube/config
sudo chown -R vagrant:vagrant /home/vagrant/.kube

echo "Agent setup with kubectl complete"
```

**Design decisions:**
- Loop di attesa: il worker può partire prima che il master abbia generato il token
- Timeout di 300s: evita attesa infinita in caso di problemi
- `K3S_URL`: Endpoint del API server del master
- `K3S_TOKEN`: Token per autenticazione
- Kubeconfig copiato da shared folder: permette kubectl dal worker

**Perché attendere il token:**
Vagrant può provisionare le VM in parallelo o in ordine non deterministico. Il worker deve attendere che il master:
1. Installi K3s
2. Generi il token
3. Salvi il token in /vagrant/token

### 4. Network Configuration

**Private Network:**
```
Network: 192.168.56.0/24
Gateway: 192.168.56.1 (VirtualBox)
Controller: 192.168.56.110
Worker: 192.168.56.111
```

**Vagrant Network Type:**
- `private_network` con IP statico
- VirtualBox crea un host-only adapter
- Le VM possono comunicare tra loro e con l'host
- Le VM hanno anche NAT per accesso internet (eth0)

**/etc/hosts Configuration:**
```
192.168.56.110 lotrapanS
192.168.56.111 lotrapanSW
```

Questo permette:
- `ping lotrapanS` invece di `ping 192.168.56.110`
- K3s può usare hostname nei certificati
- Più leggibile nei log

**Flannel Configuration:**
K3s usa Flannel in modalità VXLAN di default:
- Overlay network per pod-to-pod communication
- Ogni nodo ha un subnet /24 del pod CIDR (10.42.0.0/16)
- VXLAN tunnel sulla porta 8472 UDP
- Richiede che i nodi possano comunicare su questa porta

## Data Models

### Vagrant Configuration

```ruby
{
  machines: [
    {
      name: "lotrapanS",
      box: "bento/ubuntu-22.04",
      hostname: "lotrapanS",
      network: {
        type: "private_network",
        ip: "192.168.56.110"
      },
      provider: {
        memory: 1024,
        cpus: 1
      },
      provisioners: [
        { type: "shell", inline: "apt update && install netcat && setup hosts" },
        { type: "shell", path: "scripts/k3s_master.sh" }
      ]
    },
    {
      name: "lotrapanSW",
      box: "bento/ubuntu-22.04",
      hostname: "lotrapanSW",
      network: {
        type: "private_network",
        ip: "192.168.56.111"
      },
      provider: {
        memory: 1024,
        cpus: 1
      },
      provisioners: [
        { type: "shell", inline: "apt update && install netcat && setup hosts" },
        { type: "shell", path: "scripts/k3s_agent.sh", args: ["192.168.56.110"] }
      ]
    }
  ]
}
```

### K3s Node Token

```
Format: K10<random_string>::server:<random_string>
Example: K10a7b8c9d0e1f2g3h4i5j6k7l8m9n0o1p2::server:q3r4s5t6u7v8w9x0y1z2
Location: /var/lib/rancher/k3s/server/node-token (on controller)
Shared: /vagrant/token (accessible by worker)
```

### Kubeconfig

```yaml
apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: <base64_cert>
    server: https://192.168.56.110:6443
  name: default
contexts:
- context:
    cluster: default
    user: default
  name: default
current-context: default
users:
- name: default
  user:
    client-certificate-data: <base64_cert>
    client-key-data: <base64_key>
```

**Note:**
- Server URL deve essere 192.168.56.110 (non 127.0.0.1)
- Certificati embedded nel file
- Permette kubectl da entrambi i nodi

## Error Handling

### Token Not Available

**Scenario:** Worker non trova il token entro 300 secondi
**Cause possibili:**
- Master non è partito correttamente
- Script k3s_master.sh ha fallito
- Shared folder non è montata

**Handling:**
```bash
if [ ! -f /vagrant/token ]; then
  echo "Timeout waiting for master token"
  exit 1
fi
```

**Debug:**
```bash
# Sul master
vagrant ssh lotrapanS
sudo systemctl status k3s
ls -la /vagrant/token

# Sul worker
vagrant ssh lotrapanSW
ls -la /vagrant/
```

### Network Interface Not Detected

**Scenario:** Script non trova l'interfaccia con l'IP specificato
**Cause possibili:**
- VirtualBox non ha creato l'interfaccia
- IP non è stato assegnato ancora
- Configurazione Vagrant errata

**Handling:**
```bash
if [ -z "$IFACE" ]; then
  echo "Interface not found, using default"
  # Install without --flannel-iface flag
fi
```

**Consequence:**
- K3s usa l'interfaccia di default (probabilmente eth0 NAT)
- Flannel potrebbe non funzionare correttamente
- Pod-to-pod communication potrebbe fallire

**Debug:**
```bash
ip -4 addr show
ip route
```

### K3s Installation Failure

**Scenario:** curl o installazione K3s fallisce
**Cause possibili:**
- No internet access
- get.k3s.io non raggiungibile
- Risorse insufficienti

**Handling:**
- Script esce con exit code non-zero
- Vagrant mostra l'errore e ferma il provisioning
- User può fare `vagrant destroy` e riprovare

**Debug:**
```bash
# Test internet
ping 8.8.8.8
curl -I https://get.k3s.io

# Check resources
free -h
df -h
```

### Nodes Not Ready

**Scenario:** Nodi non diventano Ready dopo l'installazione
**Cause possibili:**
- Flannel non configurato correttamente
- Firewall blocca porte necessarie
- Risorse insufficienti

**Handling:**
```bash
kubectl get nodes
kubectl describe node lotrapanS
kubectl describe node lotrapanSW
kubectl get pods -n kube-system
```

**Common issues:**
- Flannel pods in CrashLoopBackOff: problema di networking
- CoreDNS pods pending: risorse insufficienti
- Kubelet not ready: controllare systemctl status k3s / k3s-agent

### Kubeconfig Permission Issues

**Scenario:** kubectl non funziona per l'utente vagrant
**Cause possibili:**
- Kubeconfig non copiato correttamente
- Permessi errati
- Ownership errato

**Handling:**
```bash
# Fix permissions
sudo chown -R vagrant:vagrant /home/vagrant/.kube
chmod 600 /home/vagrant/.kube/config

# Or use sudo
sudo kubectl get nodes
```

## Testing Strategy

### Manual Testing

**Test 1: VM Creation**
```bash
cd p1
vagrant up

# Expected output:
# - Both VMs created successfully
# - No provisioning errors
# - Both VMs in running state

vagrant status
# Expected: lotrapanS and lotrapanSW running
```

**Test 2: Network Connectivity**
```bash
vagrant ssh lotrapanS

# Test IP assignment
ip addr show | grep 192.168.56.110
# Expected: inet 192.168.56.110/24

# Test hostname resolution
ping -c 2 lotrapanSW
# Expected: replies from 192.168.56.111

# Test /etc/hosts
cat /etc/hosts | grep lotrapan
# Expected: both entries present

exit
```

**Test 3: K3s Installation on Controller**
```bash
vagrant ssh lotrapanS

# Check K3s service
sudo systemctl status k3s
# Expected: active (running)

# Check node status
kubectl get nodes
# Expected: lotrapanS Ready control-plane,master

# Check K3s version
kubectl version --short
# Expected: Server and Client versions displayed

exit
```

**Test 4: Token Generation**
```bash
vagrant ssh lotrapanS

# Check token file
ls -la /vagrant/token
# Expected: file exists, readable

# Check token content
cat /vagrant/token
# Expected: K10...:server:... format

# Check kubeconfig
ls -la /vagrant/kubeconfig
# Expected: file exists, readable

exit
```

**Test 5: K3s Installation on Worker**
```bash
vagrant ssh lotrapanSW

# Check K3s agent service
sudo systemctl status k3s-agent
# Expected: active (running)

# Check kubectl works
kubectl get nodes
# Expected: both nodes listed

exit
```

**Test 6: Cluster Status**
```bash
vagrant ssh lotrapanS

# Check both nodes
kubectl get nodes -o wide
# Expected:
# NAME         STATUS   ROLES                  AGE   VERSION   INTERNAL-IP       OS-IMAGE
# lotrapanS    Ready    control-plane,master   Xm    v1.x.x    192.168.56.110    Ubuntu 22.04
# lotrapanSW   Ready    <none>                 Xm    v1.x.x    192.168.56.111    Ubuntu 22.04

# Check system pods
kubectl get pods -n kube-system
# Expected: All pods Running (coredns, flannel, metrics-server, etc.)

exit
```

**Test 7: Pod Scheduling**
```bash
vagrant ssh lotrapanS

# Create a test deployment
kubectl create deployment nginx --image=nginx --replicas=2

# Wait for pods
kubectl wait --for=condition=Ready pod -l app=nginx --timeout=60s

# Check pod distribution
kubectl get pods -o wide
# Expected: Pods scheduled on both nodes

# Cleanup
kubectl delete deployment nginx

exit
```

**Test 8: kubectl from Worker**
```bash
vagrant ssh lotrapanSW

# Verify kubectl works from worker
kubectl get nodes
# Expected: both nodes listed

kubectl get pods -A
# Expected: all system pods listed

exit
```

**Test 9: Lifecycle Management**
```bash
# Stop VMs
vagrant halt

# Verify stopped
vagrant status
# Expected: both VMs stopped

# Restart
vagrant up

# Verify cluster still works
vagrant ssh lotrapanS
kubectl get nodes
# Expected: both nodes Ready

exit

# Destroy
vagrant destroy -f

# Verify removed
vagrant status
# Expected: not created
```

**Test 10: Network Interface Detection**
```bash
vagrant ssh lotrapanS

# Check which interface was detected
sudo journalctl -u k3s | grep -i "interface\|flannel"
# Expected: log showing detected interface

# Check Flannel configuration
ip addr show | grep flannel
# Expected: flannel.1 interface present

exit
```

### Validation Checklist

- [ ] Both VMs created with correct IPs
- [ ] Both VMs have 1024 MB RAM and 1 CPU
- [ ] /etc/hosts configured on both nodes
- [ ] Network connectivity between nodes
- [ ] K3s server running on controller
- [ ] K3s agent running on worker
- [ ] Token generated and shared
- [ ] Kubeconfig generated and shared
- [ ] kubectl configured on both nodes
- [ ] Both nodes in Ready state
- [ ] Controller has control-plane role
- [ ] Worker has no role or worker role
- [ ] Correct internal IPs (192.168.56.x)
- [ ] Flannel CNI configured correctly
- [ ] System pods running (coredns, flannel, etc.)
- [ ] Pods can be scheduled on both nodes
- [ ] vagrant halt/up works correctly
- [ ] vagrant destroy cleans up completely

## Directory Structure

```
p1/
├── Vagrantfile                 # Multi-machine configuration
├── scripts/
│   ├── k3s_master.sh          # Controller node setup
│   └── k3s_agent.sh           # Worker node setup
├── token                       # Generated by controller (gitignored)
└── kubeconfig                  # Generated by controller (gitignored)
```

## Implementation Notes

### Why Multi-Machine Vagrant?

Vagrant supporta configurazioni multi-machine che permettono di:
- Definire multiple VM in un singolo Vagrantfile
- Gestirle insieme con un singolo `vagrant up`
- Condividere configurazioni comuni
- Orchestrare il provisioning

### Why Shared Folder for Token?

Alternative considerate:
1. **SSH copy**: Complesso, richiede setup SSH tra nodi
2. **HTTP server**: Overkill per un singolo file
3. **Shared folder**: Semplice, built-in in Vagrant

La shared folder `/vagrant` è automaticamente montata su tutte le VM e punta alla directory del Vagrantfile sull'host.

### Why Wait Loop in Agent Script?

Vagrant può provisionare le VM in parallelo (dipende dalla configurazione). Anche se lotrapanS è definito prima di lotrapanSW, non c'è garanzia che il provisioning di lotrapanS finisca prima che inizi quello di lotrapanSW.

Il loop di attesa garantisce che il worker non fallisca se parte prima che il master abbia generato il token.

### Why --node-ip Flag?

Senza `--node-ip`, K3s potrebbe scegliere l'IP sbagliato:
- VM ha multiple interfacce (NAT + private network)
- K3s potrebbe scegliere l'IP della NAT (10.0.2.15)
- Altri nodi non potrebbero raggiungerlo

Specificando `--node-ip`, forziamo K3s a usare l'IP della private network.

### Why --flannel-iface Flag?

Stesso problema di --node-ip ma per Flannel:
- Flannel crea tunnel VXLAN tra nodi
- Deve usare l'interfaccia della private network
- Altrimenti i pod non possono comunicare tra nodi

### Why sed on Kubeconfig?

Il kubeconfig generato da K3s ha:
```yaml
server: https://127.0.0.1:6443
```

Questo funziona solo sul controller. Per usare kubectl dal worker o dall'host, serve:
```yaml
server: https://192.168.56.110:6443
```

Il sed sostituisce 127.0.0.1 con l'IP reale.

### Resource Allocation

1024 MB per nodo sono sufficienti per:
- **Controller**: K3s server (~400 MB) + system pods (~200 MB) + OS (~200 MB) = ~800 MB
- **Worker**: K3s agent (~100 MB) + system pods (~100 MB) + OS (~200 MB) = ~400 MB

Per workload reali, aumentare a 2048 MB o più.

### K3s vs K8s

K3s è perfetto per questo progetto perché:
- Installazione con un singolo comando
- Binario singolo (~50 MB vs ~1 GB di K8s)
- Embedded etcd (no cluster etcd separato)
- Include Traefik, CoreDNS, Flannel di default
- Stesso API di Kubernetes

Per produzione enterprise, si userebbe kubeadm o managed Kubernetes (EKS, GKE, AKS).
